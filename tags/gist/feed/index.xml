<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>gist on Daniel E. Cook</title>
    <link>https://www.danielecook.com/tags/gist/</link>
    <description>Recent content in gist on Daniel E. Cook</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.</copyright>
    <lastBuildDate>Fri, 28 Jun 2019 00:26:29 +0100</lastBuildDate>
    
	<atom:link href="https://www.danielecook.com/tags/gist/feed/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>A bash alias for Microsoft Excel (Mac only)</title>
      <link>https://www.danielecook.com/a-bash-alias-for-microsoft-excel-mac-only/</link>
      <pubDate>Fri, 28 Jun 2019 00:26:29 +0100</pubDate>
      
      <guid>https://www.danielecook.com/a-bash-alias-for-microsoft-excel-mac-only/</guid>
      <description>Years ago I wrote a function for opening excel from R. While I would never use Excel for data analysis, it turns out it&amp;rsquo;s pretty good for sorting and browsing data. Thats why I wrote a simple bash alias for opening up text documents from the terminal.
function excel() { tmp=`mktemp` out=${1} cat ${out} &amp;gt; $tmp open -a &amp;quot;Microsoft Excel&amp;quot; $tmp }  Usage:
cat spreadsheet.tsv | excel  </description>
    </item>
    
    <item>
      <title>Useful Nextflow bash functions for SLURM</title>
      <link>https://www.danielecook.com/useful-nextflow-bash-functions-for-slurm/</link>
      <pubDate>Fri, 21 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielecook.com/useful-nextflow-bash-functions-for-slurm/</guid>
      <description>If you use Nextflow on a cluster with the SLURM scheduler, then these bash functions may be useful to you and worth sticking in your .bashrc.
# Shortcut for going to work directories # Usage: gw &amp;lt;workdir pattern&amp;gt; # Replace the work directory below as needed # Where workdir pattern is something like &amp;#34;ab/afedeu&amp;#34; function gw { path=`ls --color=none -d /path/to/work/directory/$1*` cd $path } # sq squeue alternative # Outputs more complete information about jobs including the work directory function sq() { squeue --user `whoami` --format=&amp;#39;%.</description>
    </item>
    
    <item>
      <title>memoise: Caching in the cloud</title>
      <link>https://www.danielecook.com/memoise-caching-in-the-cloud/</link>
      <pubDate>Wed, 27 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielecook.com/memoise-caching-in-the-cloud/</guid>
      <description>Update: 2019-06-22 Based on my suggestions, out-of-memory caching was implemented in the &amp;ldquo;official&amp;rdquo; memoise package here. The memoise package now caches based on files and AWS.
Original Post Memoisation is a technique for caching the results of functions based on inputs. For example, the following function calculates the fibonnaci sequence in R.
fib &amp;lt;- function(n) { if (n &amp;lt; 2) return(1) fib(n - 2) + fib(n - 1) }  This is an innefficient way of calculating values of the fibonnacci sequence.</description>
    </item>
    
    <item>
      <title>Split a GFF File into Individual Features</title>
      <link>https://www.danielecook.com/split-a-gff-file-into-individual-features/</link>
      <pubDate>Sun, 25 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielecook.com/split-a-gff-file-into-individual-features/</guid>
      <description>The General Feature Format is a widely used format for annotating genome sequences. If indexed with tabix, gff files can be viewed in IGV or elsewhere. While features are organized in a nested manner (e.g. genes &amp;gt; exons &amp;gt; variant), you can pull out the individual types and index them, or combine only a few for viewing in your genome browser.
I was working with wormbase annotation files, which combine all the different types of features together (genes, ncRNA, mRNA, binding site, operon, G Quartets, piRNAs, etc).</description>
    </item>
    
    <item>
      <title>Aggregate FastQC Reports</title>
      <link>https://www.danielecook.com/aggregate-fastqc-reports/</link>
      <pubDate>Sun, 28 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielecook.com/aggregate-fastqc-reports/</guid>
      <description>Update: MultiQC (2019-06-21) After I originally published this script for aggregating FASTQC reports, MultiQC was published by Phil Ewels. MultiQC aggregates quality-control and other associated data from sequencing tools into an interactive report. Instead of the script below, you can simply run:
# Run this command where your *_fastqc.zip files are multiqc .  This will output a repor that looks like this:
Publication
 MultiQC: Summarize analysis results for multiple tools and samples in a single report  Philip Ewels, Måns Magnusson, Sverker Lundin and Max Käller  Bioinformatics (2016)  doi: 10.</description>
    </item>
    
    <item>
      <title>Downgrade a VCF for viewing in IGV (4.2 &gt; 4.1)</title>
      <link>https://www.danielecook.com/downgrade-a-vcf-for-viewing-in-igv-4.2-4.1/</link>
      <pubDate>Mon, 15 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielecook.com/downgrade-a-vcf-for-viewing-in-igv-4.2-4.1/</guid>
      <description>Update: You probably no longer need this (2019-06-24) If you are using up to date software then you probably do not need to worry about downgrading a VCF file.
Original Post (2014-12-15) If you are using the new version of bcftools, and you frequently use IGV to view variants you may have run into issues loading the file in IGV. IGV currently does not support VCF version 4.2. However, I&amp;#8217;ve been able to tweak the headers of newer VCF files to allow these variants to be viewable in IGV again.</description>
    </item>
    
    <item>
      <title>Rename Samples within a VCF/BCF</title>
      <link>https://www.danielecook.com/rename-samples-within-a-vcf/bcf/</link>
      <pubDate>Fri, 05 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielecook.com/rename-samples-within-a-vcf/bcf/</guid>
      <description>Update: Use bcftools (2019-06-21) Since this post was originally written, bcftools has added a command for renaming samples called reheader which allows sample names to be easily modified.
Original Post (2014-12-05) These two simple bash functions make it easy to rename samples within a bcf file by using the filename given (if it is a single sample file) or adding a prefix to all samples. This is useful if you want to merge bcf files where the sample names are identical in both (for comparison purposes).</description>
    </item>
    
    <item>
      <title>From SRA Project to FASTQ</title>
      <link>https://www.danielecook.com/from-sra-project-to-fastq/</link>
      <pubDate>Sat, 25 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielecook.com/from-sra-project-to-fastq/</guid>
      <description>Original Post (2014-10-25) The Sequence Read Archive (SRA) contains sequence data from scientific studies stored in a special &amp;#8216;sra&amp;#8217; format. Data is stored in a hierarchical format:
Project ▸ Study ▸ Sample ▸ Experiment ▸ Run
Recently, I had to use the SRA to download all of the sequence data for a given project. This required querying the SRA database for all the runs in a sequencing project and converting them to FASTQs.</description>
    </item>
    
    <item>
      <title>Calculate Depth and Breadth of Coverage From a bam File</title>
      <link>https://www.danielecook.com/calculate-depth-and-breadth-of-coverage-from-a-bam-file/</link>
      <pubDate>Sat, 20 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielecook.com/calculate-depth-and-breadth-of-coverage-from-a-bam-file/</guid>
      <description>Original Post What is the difference between depth and coverage in sequencing experiments? Actually &amp;#8211; they refer to the same thing, the average number of reads aligned to an individual base. Previously, I had thought coverage referred to the percentage of the genome with aligned reads to it; however the more appropriate term for this is breadth of coverage. This paper more precisely defines what breadth of coverage and depth of coverage mean.</description>
    </item>
    
    <item>
      <title>Generate a bedfile of masked ranges a fasta file</title>
      <link>https://www.danielecook.com/generate-a-bedfile-of-masked-ranges-a-fasta-file/</link>
      <pubDate>Mon, 15 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielecook.com/generate-a-bedfile-of-masked-ranges-a-fasta-file/</guid>
      <description>If you are calling variants as part of a NGS experiment, you likely are considering filters such as depth, quality, and filtering low complexity regions from the variant dataset. Programs such as repeatmasker are used to identify low complexity regions, replacing repetitive sequences with N&amp;rsquo;s. Repetitive regions have a tendency to be aligned with inappropriate reads and results in false positives.
If you&amp;rsquo;ve been provided with or have generated a masked fasta file for a given genome, you can use the following script convert a masked fasta (left) into a bed file (right) with the masked ranges.</description>
    </item>
    
    <item>
      <title>Generate fasta sequence lengths</title>
      <link>https://www.danielecook.com/generate-fasta-sequence-lengths/</link>
      <pubDate>Wed, 13 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielecook.com/generate-fasta-sequence-lengths/</guid>
      <description>This one liner:
cat file.fa | awk &#39;$0 ~ &amp;quot;&amp;gt;&amp;quot; {if (NR &amp;gt; 1) {print c;} c=0;printf substr($0,2,100) &amp;quot;\t&amp;quot;; } $0 !~ &amp;quot;&amp;gt;&amp;quot; {c+=length($0);} END { print c; }&#39;  Takes a fasta file as input:
&amp;gt;EF491733 tcagattcaaacaccgacgacgatgacgtggcaaagtctcgacgtgtgcg caattcgtgtatgtgtccagcaggacctcccggagaacgcggaccagtag gaccaccaggtctacggggatcgccaggatggcct &amp;gt;EF491734 tcacagggaatgaaggcactgttcgacttgatcgctttgagaccaagacc cgtggcaattctcggagggcaatgcactgaagtgaacgagccaatagcga tggcgctcaagtattggcaaatcgtgcaattatcctatgcggagacacat gccaa &amp;gt;EF491735 gtcttgcatgacccaaaaggctcctgctcttctgtttcttcttccaatac atccttctaaccagttggaagggttgacgtatcaagacttcctgcatcaa aacttcttgaatttgccttcatttgtcgcaattgtgcagc &amp;gt;EF491736 taaatggaaggaatcacttggcgctgaagaatttgctctccgcacagctt aatcagactggaactccaatggttaatccaatgatggctttacaacaaca agcggccgcagtaaacctgattcccaacacaccaatttacccaccc &amp;gt;EF491737 actctcgcaatcgtctctccccaaatgatgttaacatcactagaaatgac aaccgaacatatagcccagtcactcctcgtatcacaacaagtgagcggac agtaacaccggaacagcggtcgccgggtcgaaaagcgttcgaaaccattc &amp;gt;EF491738 tccctcgttcattcacaacaaaggaaaagcaaactatgggccattcattg ttgaaattatgaactatcatcagtattctgcaatgacaagtcatatggtc aaagtaatgaaacggccccaccaggttccgccaatgaaggtcgaccctga gg &amp;gt;EF491739 tccttccaactgttgccaactttccaactacaagacacactgaaccagaa actacgcggagacctctgtcgccttcaaaaatgacaccttctcttccttc tcctaccaccaccactttgcctgttttctttttgtcacaaatcactgacg gcgatgaatcagaagatgaa  Outputs sequence name and length:</description>
    </item>
    
    <item>
      <title>Visualizing Pairwise Queries in R</title>
      <link>https://www.danielecook.com/visualizing-pairwise-queries-in-r/</link>
      <pubDate>Sat, 02 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielecook.com/visualizing-pairwise-queries-in-r/</guid>
      <description>You can look for interesting associations between sets of search terms on PubMed by comparing how often two terms co-occur. The code below returns the number of publications where both terms are mentioned, acting as a rough estimate for how associated they are (at least, in the scholarly world).
In the example below, I show the results from organisms x diseases/disease-associated terms which is an imperfect look at how various terms estimate of how much each disease is studied in a given organism.</description>
    </item>
    
    <item>
      <title>How to plot all of your Runkeeper Data</title>
      <link>https://www.danielecook.com/how-to-plot-all-of-your-runkeeper-data/</link>
      <pubDate>Fri, 30 May 2014 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielecook.com/how-to-plot-all-of-your-runkeeper-data/</guid>
      <description>Runs in Iowa City  Running and Biking in Chicago   If you use runkeeper and pay for a yearly subscription (runkeeper elite), you can export your data and plot all of your activities simultaneously using R. I&amp;#8217;ve written a script for doing so (Special thanks to flowing data which has a tutorial that helped with a few key parts of this).
The script does a few unique things.</description>
    </item>
    
    <item>
      <title>Double Checking FASTQs</title>
      <link>https://www.danielecook.com/double-checking-fastqs/</link>
      <pubDate>Sat, 24 May 2014 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielecook.com/double-checking-fastqs/</guid>
      <description>When you have performed a sequencing project, quality control is one of the first things you will need to do. Unfortunately, sample mix-ups and other issues can and do happen. Systematic biases can also occur by machine and lane.
This script will extracting basic information from a set of FASTQs and output it to summary file (fastq_summary.txt). This will work with demultiplexed FASTQs generated by Illumina machines that appear in the following format:</description>
    </item>
    
    <item>
      <title>Downloading and storing bioinformatic databases locally</title>
      <link>https://www.danielecook.com/downloading-and-storing-bioinformatic-databases-locally/</link>
      <pubDate>Mon, 20 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielecook.com/downloading-and-storing-bioinformatic-databases-locally/</guid>
      <description>If you need to annotate biological data there are plenty of resources online (UCSC Genome Browser, BioMart), and plenty of programmatic tools to interact with these databases as well. But if you are going to be annotating a large dataset (like ChIP-Seq or RNA-Seq data) &amp;#8211; you will probably not want to rely on web based services because a) It is inefficient b) You may get throttled or banned.
If you use python, it is easy to download and store data in an SQlite database.</description>
    </item>
    
    <item>
      <title>Export excel worksheets as individual CSVs</title>
      <link>https://www.danielecook.com/export-excel-worksheets-as-individual-csvs/</link>
      <pubDate>Sat, 09 Nov 2013 22:45:53 +0000</pubDate>
      
      <guid>https://www.danielecook.com/export-excel-worksheets-as-individual-csvs/</guid>
      <description>Original Post (2013-11-09) If you need to work with data spread across a bunch of worksheets within an excel workbook, but you don&amp;#8217;t want to do so in Microsoft Excel &amp;#8211; here is a python script for extracting each individual workbook as a csv and exporting them all to a folder.
import xlrd # pip install xlrd import csv import os def export_workbook(filename): # Open workbook for initial extraction workbook = xlrd.</description>
    </item>
    
    <item>
      <title>Fetch Data from UCSC Genome Browser</title>
      <link>https://www.danielecook.com/fetch-data-from-ucsc-genome-browser/</link>
      <pubDate>Sun, 03 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielecook.com/fetch-data-from-ucsc-genome-browser/</guid>
      <description>Original Post (2013-11-03) Previously, I&amp;#8217;ve shown that you can use a mysql database browser (e.g. Sequel Pro) to access and browse the UCSC Genome Browser MySQL database.
If you have a small dataset that you would like to annotate, you can write SQL statements to fetch data. Below I show how you can use python to fetch genome coordinates by specifying gene and genome build.
# Note: Requires mysqldb; install using: # pip install MySQL-python from MySQLdb.</description>
    </item>
    
    <item>
      <title>A function for retrieving SNP data from Entrez using BioPython</title>
      <link>https://www.danielecook.com/a-function-for-retrieving-snp-data-from-entrez-using-biopython/</link>
      <pubDate>Thu, 06 Jun 2013 00:00:00 +0000</pubDate>
      
      <guid>https://www.danielecook.com/a-function-for-retrieving-snp-data-from-entrez-using-biopython/</guid>
      <description>Biopython is a great tool for interacting with biological databases. I use it to retrieve records from NCBI&amp;#8217;s Entrez databases including Pubmed.
Unfortunately &amp;#8211; one notable database biopython has trouble working with is the SNP database. This is due to the Bio.Entrez parser being unable to handle the XML returned from this database. One solution is to use a built in Python XML parser, but I thought I&amp;#8217;d try to come up with an easier solution.</description>
    </item>
    
    <item>
      <title>Django models for Chado</title>
      <link>https://www.danielecook.com/django-models-for-chado/</link>
      <pubDate>Wed, 09 Jan 2013 22:45:53 +0000</pubDate>
      
      <guid>https://www.danielecook.com/django-models-for-chado/</guid>
      <description>Original Post (2013-01-09) Here is my first stab at models for django of the Chado database schema
Gist → Chado models
Update: machado (2019-06-20) I originally put together a set of models for Chado in January of 2013. I later moved on from this work when I started graduate school - but a group incorporated a little bit of this work in the development of their framework for storing, searching and visualizing biological data called machado.</description>
    </item>
    
  </channel>
</rss>